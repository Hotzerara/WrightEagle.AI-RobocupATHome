import wave
import numpy as np
import pyaudio
import samplerate
from faster_whisper import WhisperModel
import torch
import torchaudio
from silero_vad import load_silero_vad, VADIterator
import collections

from config import WHISPER_MEDIUM, WHISPER_LARGE


# ============= æŸ¥æ‰¾æŒ‡å®šéº¦å…‹é£ =============
def find_input_device(name_part: str):
    p = pyaudio.PyAudio()
    for i in range(p.get_device_count()):
        info = p.get_device_info_by_index(i)
        if name_part.lower() in info["name"].lower() and info["maxInputChannels"] > 0:
            print(f"âœ“ æ‰¾åˆ°éº¦å…‹é£: {info['name']} (index={i})")
            rate = int(info.get("defaultSampleRate", 48000))
            return i, rate
    print("âš  æœªæ‰¾åˆ°æŒ‡å®šéº¦å…‹é£ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡")
    index = p.get_default_input_device_info()["index"]
    info = p.get_device_info_by_index(index)
    return index, int(info.get("defaultSampleRate", 48000))


class SpeechRecognizer:
    def __init__(self, mic_name="Newmine"):
        self.model_path = WHISPER_MEDIUM

        print("åŠ è½½ Whisper æ¨¡å‹ä¸­...")
        self.model = WhisperModel(
            self.model_path,
            device="cpu",
            compute_type="int8"
        )
        print("âœ“ Whisper æ¨¡å‹åŠ è½½å®Œæˆï¼")

        # åŠ è½½ Silero VAD æ¨¡å‹
        print("åŠ è½½ Silero VAD æ¨¡å‹ä¸­...")
        self.vad_model = load_silero_vad()
        print("âœ“ Silero VAD æ¨¡å‹åŠ è½½å®Œæˆï¼")

        # Whisper å›ºå®š 16k é‡‡æ ·ç‡
        self.target_rate = 16000

        # ===== æŸ¥æ‰¾éº¦å…‹é£ =====
        self.p = pyaudio.PyAudio()
        self.device_index, self.device_rate = find_input_device(mic_name)
        print(f"ğŸ¤ éº¦å…‹é£é‡‡æ ·ç‡: {self.device_rate} Hz")

        # æ¯å¸§ 32ms ä¿è¯åœ¨16k é‡‡æ ·ä¸‹æ¯å¸§æœ‰512ä¸ªé‡‡æ ·ç‚¹æ»¡è¶³vadæ¨¡å‹çš„éœ€æ±‚
        self.frame_duration_ms = 32
        # è®¾å¤‡ä¾§æ¯å¸§é‡‡æ ·ç‚¹æ•°
        self.frame_samples_device = int(self.device_rate * self.frame_duration_ms / 1000)
        # 16k ä¾§æ¯å¸§é‡‡æ ·ç‚¹æ•°
        self.frame_samples_16k = int(self.target_rate * self.frame_duration_ms / 1000)

        # ===== æ‰“å¼€éº¦å…‹é£æµ =====
        self.stream = self.p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.device_rate,
            input=True,
            frames_per_buffer=self.frame_samples_device,
            input_device_index=self.device_index
        )

        # å½•éŸ³ä¿å­˜ï¼ˆå¯é€‰ï¼‰
        self.wav_file = wave.open("recorded_audio.wav", "wb")
        self.wav_file.setnchannels(1)
        self.wav_file.setsampwidth(2)
        self.wav_file.setframerate(self.device_rate)

        # ===== è¯­éŸ³ç¼“å†²åŒºï¼ˆ16kï¼‰=====
        self.buffer_16k = np.array([], dtype=np.float32)

        # VAD Iterator ç”¨äºæµå¼æ£€æµ‹
        self.vad_iterator = VADIterator(self.vad_model, threshold=0.5)

        # çŠ¶æ€è·Ÿè¸ª
        self.is_speaking = False

        # å¯åŠ¨æ—¶ç®€å•æ£€æµ‹ç¯å¢ƒå™ªå£°
        self.noise_floor = self.detect_noise_floor()
        print(f"ç¯å¢ƒå™ªå£°èƒ½é‡ä¼°è®¡: {self.noise_floor:.6f}")

    # ============= è¾…åŠ©å‡½æ•° =============
    @staticmethod
    def rms(audio):
        """è®¡ç®—çŸ­æ—¶èƒ½é‡"""
        if len(audio) == 0:
            return 0.0
        return float(np.sqrt(np.mean(audio ** 2)))

    def detect_noise_floor(self):
        """å¯åŠ¨æ—¶é‡‡å‡ å¸§ä¼°è®¡å™ªå£°èƒ½é‡"""
        print("æ­£åœ¨æ£€æµ‹ç¯å¢ƒå™ªå£°ï¼ˆè¯·æš‚æ—¶ä¸è¦è¯´è¯ï¼‰...")
        samples = []
        num_frames = 10  # 10 * 30ms = 300ms
        for _ in range(num_frames):
            data = self.stream.read(self.frame_samples_device, exception_on_overflow=False)
            # é¡ºä¾¿å†™è¿› wavï¼Œé¿å…ä¸¢å¤´å‡ å¸§
            self.wav_file.writeframes(data)

            audio_dev = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
            audio_16k = samplerate.resample(
                audio_dev,
                self.target_rate / self.device_rate,
                "sinc_best"
            )
            samples.append(self.rms(audio_16k))

        floor = float(np.mean(samples)) if samples else 0.0
        return floor

    # ============= ä¸»å¾ªç¯ =============
    def start_listening(self):
        print("å¼€å§‹å®æ—¶è¯­éŸ³è¯†åˆ« (Silero VAD)...\n")

        try:
            while True:
                # æŒ‰å¸§è¯»å–
                data = self.stream.read(self.frame_samples_device, exception_on_overflow=False)
                # ä¿å­˜åˆ° wav
                self.wav_file.writeframes(data)

                # è½¬æˆ float32 [-1, 1]ï¼ˆè®¾å¤‡é‡‡æ ·ç‡ï¼‰
                audio_dev = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0

                # é‡é‡‡æ ·åˆ° 16kï¼Œç”¨äº VAD + Whisper
                audio_16k = samplerate.resample(
                    audio_dev,
                    self.target_rate / self.device_rate,
                    "sinc_best"
                )

                # ç¡®ä¿é•¿åº¦æ˜¯æˆ‘ä»¬æœŸæœ›çš„å¸§å¤§å°
                if len(audio_16k) > self.frame_samples_16k:
                    audio_16k = audio_16k[:self.frame_samples_16k]
                elif len(audio_16k) < self.frame_samples_16k:
                    pad_len = self.frame_samples_16k - len(audio_16k)
                    audio_16k = np.concatenate(
                        [audio_16k, np.zeros(pad_len, dtype=np.float32)]
                    )

                # ä½¿ç”¨ Silero VAD æ£€æµ‹
                speech_dict = self.vad_iterator(audio_16k, return_seconds=False)

                if speech_dict is not None:
                    if 'start' in speech_dict:
                        # æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹
                        print("å¼€å§‹è¯´è¯...")
                        self.is_speaking = True
                        self.buffer_16k = np.array([], dtype=np.float32)  # é‡ç½®ç¼“å†²åŒº

                    if 'end' in speech_dict:
                        # æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
                        print("è¯´è¯ç»“æŸï¼Œå¼€å§‹è¯†åˆ«...")
                        self.is_speaking = False
                        if len(self.buffer_16k) > int(self.target_rate * 0.1):  # è‡³å°‘0.1ç§’
                            self.transcribe_buffer()

                # å¦‚æœæ­£åœ¨è¯´è¯ä¸­ï¼Œå°†éŸ³é¢‘åŠ å…¥ç¼“å†²åŒº
                if self.is_speaking:
                    self.buffer_16k = np.concatenate([self.buffer_16k, audio_16k])

        except KeyboardInterrupt:
            print("è¯†åˆ«å·²åœæ­¢")
            # åœæ­¢å‰æŠŠæœ€åä¸€æ®µè¯´å®Œçš„è¯ä¹Ÿè¯†åˆ«ä¸€ä¸‹
            if self.is_speaking and len(self.buffer_16k) > int(self.target_rate * 0.1):
                self.transcribe_buffer()
        finally:
            self.cleanup()

    # ============= é€ç»™ Whisper è¯†åˆ« =============
    def transcribe_buffer(self):
        audio = self.buffer_16k.astype(np.float32)
        if len(audio) == 0:
            return

        # è¿™é‡Œä¿æŒä½ åŸæ¥çš„è®¾ç½®ï¼šè‹±è¯­ï¼Œbeam searchï¼Œä¸è¦æ—¶é—´æˆ³
        segments, _ = self.model.transcribe(
            audio,
            beam_size=5,
            language="en",
            without_timestamps=True
        )

        text = "".join(seg.text.strip() + " " for seg in segments).strip()
        if text:
            print("è¯†åˆ«:", text)

    # ============= æ¸…ç†èµ„æº =============
    def cleanup(self):
        self.stream.stop_stream()
        self.stream.close()
        self.p.terminate()
        self.wav_file.close()
        print("å½•éŸ³å·²ä¿å­˜ä¸º recorded_audio.wav")


if __name__ == "__main__":
    recognizer = SpeechRecognizer("Newmine")
    recognizer.start_listening()

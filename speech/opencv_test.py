import wave
import numpy as np
import pyaudio
import samplerate
from faster_whisper import WhisperModel
import cv2
from silero_vad import load_silero_vad, VADIterator
import threading
import queue
import time

from config import WHISPER_MEDIUM


# ============= æŸ¥æ‰¾æŒ‡å®šéº¦å…‹é£ =============
def find_input_device(name_part: str):
    p = pyaudio.PyAudio()
    for i in range(p.get_device_count()):
        info = p.get_device_info_by_index(i)
        if name_part.lower() in info["name"].lower() and info["maxInputChannels"] > 0:
            print(f"âœ“ æ‰¾åˆ°éº¦å…‹é£: {info['name']} (index={i})")
            rate = int(info.get("defaultSampleRate", 48000))
            return i, rate
    print("âš  æœªæ‰¾åˆ°æŒ‡å®šéº¦å…‹é£ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡")
    index = p.get_default_input_device_info()["index"]
    info = p.get_device_info_by_index(index)
    return index, int(info.get("defaultSampleRate", 48000))


class SpeechRecognizer:
    def __init__(self, mic_name="Newmine"):
        self.model_path = WHISPER_MEDIUM

        print("åŠ è½½ Whisper æ¨¡å‹ä¸­...")
        self.model = WhisperModel(
            self.model_path,
            device="cpu",
            compute_type="int8"
        )
        print("âœ“ Whisper æ¨¡å‹åŠ è½½å®Œæˆï¼")

        # åŠ è½½ Silero VAD æ¨¡å‹
        print("åŠ è½½ Silero VAD æ¨¡å‹ä¸­...")
        self.vad_model = load_silero_vad()
        print("âœ“ Silero VAD æ¨¡å‹åŠ è½½å®Œæˆï¼")

        # Whisper å›ºå®š 16k é‡‡æ ·ç‡
        self.target_rate = 16000

        # ===== æŸ¥æ‰¾éº¦å…‹é£ =====
        self.p = pyaudio.PyAudio()
        self.device_index, self.device_rate = find_input_device(mic_name)
        print(f"ğŸ¤ éº¦å…‹é£é‡‡æ ·ç‡: {self.device_rate} Hz")

        # æ¯å¸§ 32ms ä¿è¯åœ¨16k é‡‡æ ·ä¸‹æ¯å¸§æœ‰512ä¸ªé‡‡æ ·ç‚¹æ»¡è¶³vadæ¨¡å‹çš„éœ€æ±‚
        self.frame_duration_ms = 32
        # è®¾å¤‡ä¾§æ¯å¸§é‡‡æ ·ç‚¹æ•°
        self.frame_samples_device = int(self.device_rate * self.frame_duration_ms / 1000)
        # 16k ä¾§æ¯å¸§é‡‡æ ·ç‚¹æ•°
        self.frame_samples_16k = int(self.target_rate * self.frame_duration_ms / 1000)

        # ===== æ‰“å¼€éº¦å…‹é£æµ =====
        self.stream = self.p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.device_rate,
            input=True,
            frames_per_buffer=self.frame_samples_device,
            input_device_index=self.device_index
        )

        # å½•éŸ³ä¿å­˜ï¼ˆå¯é€‰ï¼‰
        self.wav_file = wave.open("recorded_audio.wav", "wb")
        self.wav_file.setnchannels(1)
        self.wav_file.setsampwidth(2)
        self.wav_file.setframerate(self.device_rate)

        # ===== è¯­éŸ³ç¼“å†²åŒºï¼ˆ16kï¼‰=====
        self.buffer_16k = np.array([], dtype=np.float32)

        # VAD Iterator ç”¨äºæµå¼æ£€æµ‹
        self.vad_iterator = VADIterator(self.vad_model, threshold=0.5)

        # çŠ¶æ€è·Ÿè¸ª
        self.is_speaking = False
        self.last_transcription = ""
        self.current_status = "Listening..."
        self.current_energy = 0.0

        # å¯åŠ¨æ—¶ç®€å•æ£€æµ‹ç¯å¢ƒå™ªå£°
        self.noise_floor = self.detect_noise_floor()
        print(f"ç¯å¢ƒå™ªå£°èƒ½é‡ä¼°è®¡: {self.noise_floor:.6f}")

        # OpenCVå¯è§†åŒ–å‚æ•°
        self.window_name = "Real-time Speech Recognition"
        self.window_width = 800
        self.window_height = 600

        # è¯†åˆ«é˜Ÿåˆ—å’Œçº¿ç¨‹
        self.recognition_queue = queue.Queue()
        self.recognition_thread = None
        self.recognition_active = False

    # ============= è¾…åŠ©å‡½æ•° =============
    @staticmethod
    def rms(audio):
        """è®¡ç®—çŸ­æ—¶èƒ½é‡"""
        if len(audio) == 0:
            return 0.0
        return float(np.sqrt(np.mean(audio ** 2)))

    def detect_noise_floor(self):
        """å¯åŠ¨æ—¶é‡‡å‡ å¸§ä¼°è®¡å™ªå£°èƒ½é‡"""
        print("æ­£åœ¨æ£€æµ‹ç¯å¢ƒå™ªå£°ï¼ˆè¯·æš‚æ—¶ä¸è¦è¯´è¯ï¼‰...")
        samples = []
        num_frames = 10  # 10 * 32ms = 320ms
        for _ in range(num_frames):
            data = self.stream.read(self.frame_samples_device, exception_on_overflow=False)
            # é¡ºä¾¿å†™è¿› wavï¼Œé¿å…ä¸¢å¤´å‡ å¸§
            self.wav_file.writeframes(data)

            audio_dev = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
            audio_16k = samplerate.resample(
                audio_dev,
                self.target_rate / self.device_rate,
                "sinc_best"
            )
            samples.append(self.rms(audio_16k))

        floor = float(np.mean(samples)) if samples else 0.0
        return floor

    def recognition_worker(self):
        """åå°è¯†åˆ«å·¥ä½œçº¿ç¨‹"""
        while self.recognition_active:
            try:
                # ä»é˜Ÿåˆ—è·å–éœ€è¦è¯†åˆ«çš„éŸ³é¢‘æ•°æ®
                audio_data = self.recognition_queue.get(timeout=0.1)
                if audio_data is None:  # åœæ­¢ä¿¡å·
                    break

                # æ‰§è¡Œè¯†åˆ«
                segments, _ = self.model.transcribe(
                    audio_data,
                    beam_size=5,
                    language="en",
                    without_timestamps=True
                )

                text = "".join(seg.text.strip() + " " for seg in segments).strip()
                if text:
                    print("è¯†åˆ«:", text)
                    self.last_transcription = text
                    self.current_status = f"Result: {text[:50]}..."
                    if len(text) > 50:
                        self.current_status += "..."
                else:
                    self.current_status = "No speech detected"

            except queue.Empty:
                continue
            except Exception as e:
                print(f"è¯†åˆ«å‡ºé”™: {e}")
                self.current_status = "Recognition error"

        print("è¯†åˆ«çº¿ç¨‹å·²é€€å‡º")

    def start_recognition_thread(self):
        """å¯åŠ¨è¯†åˆ«çº¿ç¨‹"""
        self.recognition_active = True
        self.recognition_thread = threading.Thread(target=self.recognition_worker, daemon=True)
        self.recognition_thread.start()

    def submit_for_recognition(self, audio_data):
        """æäº¤éŸ³é¢‘æ•°æ®è¿›è¡Œè¯†åˆ«"""
        # æ¸…ç©ºé˜Ÿåˆ—ï¼Œé¿å…ç§¯å‹æ—§æ•°æ®
        with self.recognition_queue.mutex:
            self.recognition_queue.queue.clear()
        self.recognition_queue.put(audio_data.copy())

    def draw_visualization(self):
        """ç»˜åˆ¶OpenCVå¯è§†åŒ–ç•Œé¢"""
        # åˆ›å»ºæ·±è‰²èƒŒæ™¯ (ç°ä»£æ„Ÿ)
        img = np.zeros((self.window_height, self.window_width, 3), dtype=np.uint8)
        img.fill(20)  # æ·±ç°è‰²èƒŒæ™¯

        # æ·»åŠ æ¸å˜èƒŒæ™¯æ•ˆæœ
        for i in range(self.window_height):
            alpha = i / self.window_height
            color = int(20 + 10 * alpha)
            cv2.line(img, (0, i), (self.window_width, i), (color, color, color), 1)

        # === æ ‡é¢˜åŒºåŸŸ ===
        title_bg = np.zeros((80, self.window_width, 3), dtype=np.uint8)
        title_bg.fill(30)
        img[0:80, 0:self.window_width] = title_bg

        # ä¸»æ ‡é¢˜
        cv2.putText(img, "Real-time Speech Recognition", (30, 45),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 200, 255), 2)

        # å‰¯æ ‡é¢˜
        cv2.putText(img, "Silero VAD + Whisper Medium", (30, 75),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (150, 150, 200), 1)

        # === çŠ¶æ€å¡ç‰‡ ===
        card_y = 100
        card_height = 65
        card_bg = np.zeros((card_height, self.window_width - 60, 3), dtype=np.uint8)
        card_bg.fill(40)

        # æ·»åŠ å¡ç‰‡è¾¹æ¡†
        cv2.rectangle(img, (30, card_y), (self.window_width - 30, card_y + card_height),
                      (60, 60, 60), 1)
        img[card_y:card_y + card_height, 30:self.window_width - 30] = card_bg

        # çŠ¶æ€æŒ‡ç¤ºå™¨ (åŠ¨æ€é¢œè‰²)
        if self.is_speaking:
            status_color = (0, 255, 0)  # ç»¿è‰² - è¯´è¯ä¸­
            status_text = "SPEAKING"
            # æ·»åŠ è„‰åŠ¨æ•ˆæœ
            pulse = int(10 * abs(np.sin(time.time() * 8)))
            cv2.circle(img, (60, card_y + 40), 12,
                       (0, 255 - pulse, 0), -1)
        elif self.current_status.startswith("Recognizing"):
            status_color = (255, 255, 0)  # é»„è‰² - è¯†åˆ«ä¸­
            status_text = "PROCESSING"
            cv2.circle(img, (60, card_y + 40), 12,
                       (255, 255, 0), 2)
        else:
            status_color = (100, 100, 255)  # è“è‰² - ç›‘å¬ä¸­
            status_text = "LISTENING"
            cv2.circle(img, (60, card_y + 40), 10,
                       status_color, -1)

        # çŠ¶æ€æ–‡æœ¬
        cv2.putText(img, status_text, (90, card_y + 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)

        # è¯¦ç»†çŠ¶æ€
        cv2.putText(img, self.current_status, (90, card_y + 55),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)

        # === éŸ³é‡ä»ªè¡¨ ===
        meter_y = 200
        cv2.putText(img, "VOLUME LEVEL", (30, meter_y - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (150, 150, 200), 1)

        # è®¡ç®—éŸ³é‡ç™¾åˆ†æ¯” (æ›´å¹³æ»‘çš„æ˜¾ç¤º)
        energy_ratio = max(0.0, (self.current_energy - self.noise_floor) / (0.1 - self.noise_floor))
        energy_percent = min(100.0, energy_ratio * 100)

        # éŸ³é‡æ¡èƒŒæ™¯
        cv2.rectangle(img, (30, meter_y), (self.window_width - 30, meter_y + 25),
                      (50, 50, 50), -1)

        # åŠ¨æ€éŸ³é‡æ¡ (é¢œè‰²æ¸å˜)
        bar_width = int((self.window_width - 60) * energy_percent / 100)
        if energy_percent < 30:
            bar_color = (0, 200, 0)  # ç»¿è‰²
        elif energy_percent < 70:
            bar_color = (0, 200, 200)  # é»„è‰²
        else:
            bar_color = (0, 100, 255)  # çº¢è‰²

        if bar_width > 0:
            cv2.rectangle(img, (30, meter_y), (30 + bar_width, meter_y + 25),
                          bar_color, -1)

        # éŸ³é‡æ•°å€¼
        cv2.putText(img, f"{energy_percent:.1f}%", (self.window_width - 80, meter_y + 18),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        # åˆ»åº¦æ ‡è®°
        for i in range(0, 101, 25):
            x_pos = 30 + int((self.window_width - 60) * i / 100)
            cv2.line(img, (x_pos, meter_y + 25), (x_pos, meter_y + 30),
                     (150, 150, 150), 1)
            cv2.putText(img, str(i), (x_pos - 10, meter_y + 45),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)

        # === è¯†åˆ«ç»“æœåŒºåŸŸ ===
        result_y = 280
        cv2.putText(img, "TRANSCRIPTION", (30, result_y),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 200, 255), 1)

        # ç»“æœèƒŒæ™¯æ¡†
        result_bg = np.zeros((180, self.window_width - 60, 3), dtype=np.uint8)
        result_bg.fill(25)
        cv2.rectangle(img, (30, result_y + 20), (self.window_width - 30, result_y + 180),
                      (40, 40, 40), -1)
        cv2.rectangle(img, (30, result_y + 20), (self.window_width - 30, result_y + 180),
                      (80, 80, 80), 1)

        # æ˜¾ç¤ºè¯†åˆ«æ–‡æœ¬ (å¤šè¡Œæ”¯æŒ)
        y_offset = result_y + 50
        max_line_width = 60  # å­—ç¬¦æ•°é™åˆ¶

        if self.last_transcription:
            lines = []
            words = self.last_transcription.split()
            current_line = ""

            for word in words:
                test_line = current_line + word + " "
                if len(test_line) <= max_line_width:
                    current_line = test_line
                else:
                    if current_line:
                        lines.append(current_line.strip())
                    current_line = word + " "

            if current_line:
                lines.append(current_line.strip())

            # é™åˆ¶æ˜¾ç¤ºè¡Œæ•°
            lines = lines[-4:]  # åªæ˜¾ç¤ºæœ€å4è¡Œ

            for i, line in enumerate(lines):
                # æ–‡å­—é˜´å½±æ•ˆæœ
                cv2.putText(img, line, (52, y_offset + i * 35 + 2),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 0), 2)
                # ä¸»æ–‡å­—
                cv2.putText(img, line, (50, y_offset + i * 35),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.65, (173, 216, 230), 2)
        else:
            # æ— å†…å®¹æ—¶çš„æç¤º
            placeholder = "Speak to see transcription here..."
            cv2.putText(img, placeholder, (50, y_offset + 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 150), 1)

        # === åº•éƒ¨ä¿¡æ¯æ  ===
        footer_y = self.window_height - 40
        footer_bg = np.zeros((40, self.window_width, 3), dtype=np.uint8)
        footer_bg.fill(15)
        img[footer_y:footer_y + 40, 0:self.window_width] = footer_bg

        # é€€å‡ºæç¤º
        cv2.putText(img, "Press 'q' to exit",
                    (self.window_width // 2 - 100, self.window_height - 15),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)

        cv2.imshow(self.window_name, img)

        # æ£€æŸ¥é€€å‡ºé”®
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            raise KeyboardInterrupt

    # ============= ä¸»å¾ªç¯ =============
    def start_listening(self):
        print("å¼€å§‹å®æ—¶è¯­éŸ³è¯†åˆ« (Silero VAD + OpenCV å¯è§†åŒ–)...\n")
        print("OpenCVçª—å£å·²æ‰“å¼€ï¼Œè¯·å…³æ³¨ç•Œé¢ã€‚æŒ‰ 'q' é”®é€€å‡ºã€‚")

        # åˆ›å»ºOpenCVçª—å£
        cv2.namedWindow(self.window_name, cv2.WINDOW_AUTOSIZE)
        cv2.resizeWindow(self.window_name, self.window_width, self.window_height)

        # å¯åŠ¨è¯†åˆ«çº¿ç¨‹
        self.start_recognition_thread()

        try:
            while True:
                # æŒ‰å¸§è¯»å–
                data = self.stream.read(self.frame_samples_device, exception_on_overflow=False)
                # ä¿å­˜åˆ° wav
                self.wav_file.writeframes(data)

                # è½¬æˆ float32 [-1, 1]ï¼ˆè®¾å¤‡é‡‡æ ·ç‡ï¼‰
                audio_dev = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0

                # æ›´æ–°å½“å‰èƒ½é‡å€¼ç”¨äºå¯è§†åŒ–
                self.current_energy = self.rms(audio_dev)

                # é‡é‡‡æ ·åˆ° 16kï¼Œç”¨äº VAD + Whisper
                audio_16k = samplerate.resample(
                    audio_dev,
                    self.target_rate / self.device_rate,
                    "sinc_best"
                )

                # ç¡®ä¿é•¿åº¦æ˜¯æˆ‘ä»¬æœŸæœ›çš„å¸§å¤§å°
                if len(audio_16k) > self.frame_samples_16k:
                    audio_16k = audio_16k[:self.frame_samples_16k]
                elif len(audio_16k) < self.frame_samples_16k:
                    pad_len = self.frame_samples_16k - len(audio_16k)
                    audio_16k = np.concatenate(
                        [audio_16k, np.zeros(pad_len, dtype=np.float32)]
                    )

                # ä½¿ç”¨ Silero VAD æ£€æµ‹
                speech_dict = self.vad_iterator(audio_16k, return_seconds=False)

                if speech_dict is not None:
                    if 'start' in speech_dict:
                        # æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹
                        print("å¼€å§‹è¯´è¯...")
                        self.is_speaking = True
                        self.current_status = "Speaking..."
                        self.buffer_16k = np.array([], dtype=np.float32)  # é‡ç½®ç¼“å†²åŒº

                    if 'end' in speech_dict:
                        # æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
                        print("è¯´è¯ç»“æŸï¼Œå¼€å§‹è¯†åˆ«...")
                        self.is_speaking = False
                        self.current_status = "Recognizing..."
                        if len(self.buffer_16k) > int(self.target_rate * 0.1):  # è‡³å°‘0.1ç§’
                            # æäº¤éŸ³é¢‘æ•°æ®åˆ°åå°çº¿ç¨‹è¿›è¡Œè¯†åˆ«
                            self.submit_for_recognition(self.buffer_16k)
                        else:
                            self.current_status = "Too short"

                # å¦‚æœæ­£åœ¨è¯´è¯ä¸­ï¼Œå°†éŸ³é¢‘åŠ å…¥ç¼“å†²åŒº
                if self.is_speaking:
                    self.buffer_16k = np.concatenate([self.buffer_16k, audio_16k])

                # æ›´æ–°çŠ¶æ€
                if not self.is_speaking and self.current_status != "Recognizing...":
                    self.current_status = "Listening..."

                # ç»˜åˆ¶å¯è§†åŒ–ç•Œé¢
                self.draw_visualization()

        except KeyboardInterrupt:
            print("è¯†åˆ«å·²åœæ­¢")
            # åœæ­¢å‰æŠŠæœ€åä¸€æ®µè¯´å®Œçš„è¯ä¹Ÿè¯†åˆ«ä¸€ä¸‹
            if self.is_speaking and len(self.buffer_16k) > int(self.target_rate * 0.1):
                self.submit_for_recognition(self.buffer_16k)
        finally:
            # åœæ­¢è¯†åˆ«çº¿ç¨‹
            self.recognition_active = False
            # å‘é€åœæ­¢ä¿¡å·
            try:
                self.recognition_queue.put(None, timeout=0.1)
            except:
                pass
            if self.recognition_thread and self.recognition_thread.is_alive():
                self.recognition_thread.join(timeout=2)
            self.cleanup()

    # ============= é€ç»™ Whisper è¯†åˆ« =============
    def transcribe_buffer(self):
        """ä¿æŒè¿™ä¸ªæ–¹æ³•ç”¨äºå…¼å®¹æ€§ï¼Œå®é™…ä½¿ç”¨åå°çº¿ç¨‹"""
        pass

    # ============= æ¸…ç†èµ„æº =============
    def cleanup(self):
        self.stream.stop_stream()
        self.stream.close()
        self.p.terminate()
        self.wav_file.close()
        cv2.destroyAllWindows()
        print("å½•éŸ³å·²ä¿å­˜ä¸º recorded_audio.wav")


if __name__ == "__main__":
    recognizer = SpeechRecognizer("Newmine")
    recognizer.start_listening()